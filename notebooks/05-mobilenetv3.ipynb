{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14375810,"sourceType":"datasetVersion","datasetId":9180780}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\nimport time\nimport os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport cv2\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score, accuracy_score, cohen_kappa_score, hamming_loss, confusion_matrix, precision_score, recall_score\nimport matplotlib.pyplot as plt\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:48:07.200968Z","iopub.execute_input":"2026-01-03T11:48:07.201298Z","iopub.status.idle":"2026-01-03T11:48:21.403174Z","shell.execute_reply.started":"2026-01-03T11:48:07.201266Z","shell.execute_reply":"2026-01-03T11:48:21.402555Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\nimport os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport cv2\nimport seaborn as sns\nfrom tqdm import tqdm\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üöÄ Dispositivo: {device}\")\n\n# ==================== PATHS CORRETOS ====================\n\n# Base do dataset\nDATASET_BASE = \"/kaggle/input/odir5k\"\nDATASET_ROOT = \"/kaggle/input/odir5k/data/odir5k\"\n\n# Paths das splits\nTRAIN_DIR = f\"{DATASET_ROOT}/train\"\nVAL_DIR = f\"{DATASET_ROOT}/val\"\nTEST_DIR = f\"{DATASET_ROOT}/test\"\n\n# Output\nMODEL_SAVE_PATH = \"/kaggle/working\"\nCURRENT_DIR = \"/kaggle/working\"\n      \nclass ODIRDataset(Dataset):\n    \"\"\"Dataset ODIR-5K usando estrutura de pastas\"\"\"\n    \n    def __init__(self, split='train', transform=None):\n        self.split = split\n        self.transform = transform\n        self.img_dir = os.path.join(DATASET_ROOT, split)\n        \n        # Carregar metadados\n        metadata_path = os.path.join(self.img_dir, f\"{split}_metadata.csv\")\n        self.data = pd.read_csv(metadata_path)\n        self.disease_cols = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n        self.image_files = [f for f in os.listdir(self.img_dir) if f.endswith('.jpg')]\n        \n        print(f\"  üìÇ {split}: {len(self.data)} pacients, {len(self.image_files)} images\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        patient_id = str(row['ID'])\n        \n        img_path = None\n        for side in ['left', 'right']:\n            img_name = f\"{patient_id}_{side}.jpg\"\n            full_path = os.path.join(self.img_dir, img_name)\n            if os.path.exists(full_path):\n                img_path = full_path\n                break\n        \n        if img_path is None:\n            raise FileNotFoundError(f\"Imagem n√£o encontrada para ID {patient_id}\")\n        \n        image = Image.open(img_path).convert('RGB')\n        labels = torch.tensor([row[col] for col in self.disease_cols], dtype=torch.float32)\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, labels, patient_id\n\nclass CropOnly(object):\n        \"\"\"S√≥ cropping, SEM CLAHE\"\"\"\n        \n        def __call__(self, img):\n            img = np.array(img)\n            \n            # Mesmo cropping que ApplyCLAHEandCrop_Adaptive mas SEM CLAHE\n            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n            mask = gray > 10\n            if np.any(mask):\n                coords = np.argwhere(mask)\n                y0, x0 = coords.min(axis=0)\n                y1, x1 = coords.max(axis=0) + 1\n                img = img[y0:y1, x0:x1]\n            \n            return Image.fromarray(img)\nclass ApplyCLAHEandCrop_Adaptive(object):\n    \"\"\"\n    CLAHE adaptativo: s√≥ aplica em imagens de baixo contraste\n    \"\"\"\n    \n    def __init__(self):\n        self.contrast_threshold = 50   # ‚Üê OTIMIZADO!\n        self.clip_limit = 3.0          # Suave\n        self.tile_grid_size = (8, 8)\n    \n    def __call__(self, img):\n        # To numpy (opencv)\n        img = np.array(img)\n        \n        # ==================== CROPPING ====================\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray > 10\n        \n        if np.any(mask):\n            coords = np.argwhere(mask)\n            y0, x0 = coords.min(axis=0)\n            y1, x1 = coords.max(axis=0) + 1\n            img = img[y0:y1, x0:x1]\n        \n        # ==================== MEDIR CONTRASTE ====================\n        gray_cropped = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        contrast = gray_cropped.std()  # Standard deviation = contraste\n        \n        # ==================== CLAHE ADAPTATIVO ====================\n        # S√ì aplicar se contraste baixo!\n        if contrast < self.contrast_threshold:\n            # Converter para LAB\n            lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n            l, a, b = cv2.split(lab)\n            \n            # Aplicar CLAHE no canal L\n            clahe = cv2.createCLAHE(\n                clipLimit=self.clip_limit,\n                tileGridSize=self.tile_grid_size\n            )\n            l = clahe.apply(l)\n            \n            # Merge e converter de volta\n            img = cv2.merge([l, a, b])\n            img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n        \n        # To PIL\n        return Image.fromarray(img)\n\ndef get_train_transform():\n    return transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\ndef get_train_transform_CLAHE():\n    return transforms.Compose([\n        ApplyCLAHEandCrop_Adaptive(),\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\ndef get_val_test_transform():\n    return transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\ndef get_val_test_transform_CLAHE():\n    return transforms.Compose([\n        ApplyCLAHEandCrop_Adaptive(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n        \ndef get_v1_baseline_transform():\n    \"\"\"V1: Baseline puro (SEM crop, SEM aug, SEM CLAHE)\"\"\"\n    return transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n\n\ndef get_v2_crop_only_transform():\n    \"\"\"V2: S√≥ cropping (SEM aug, SEM CLAHE)\"\"\"\n    return transforms.Compose([\n        CropOnly(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n\n\ndef get_v3_crop_aug_transform():\n    \"\"\"V3: Cropping + Augmentation (SEM CLAHE)\"\"\"\n    return transforms.Compose([\n        CropOnly(),\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(degrees=5),\n        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n\n\ndef get_v4_full_pipeline_transform():\n    \"\"\"V4: Full Pipeline (Crop + Aug + CLAHE)\"\"\"\n    return transforms.Compose([\n        ApplyCLAHEandCrop_Adaptive(),\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n# --- Verifica√ß√£o de Seguran√ßa e Execu√ß√£o ---\nif not os.path.exists(DATASET_ROOT):\n    print(f\"‚ùå ERRO: Dataset n√£o encontrado em {DATASET_ROOT}\")\nelse:\n    # 1. Criar Datasets Reais\n    train_dataset_basic = ODIRDataset('train', transform=get_train_transform())\n    train_dataset_clahe = ODIRDataset('train', transform=get_train_transform_CLAHE())\n    \n    val_dataset = ODIRDataset('val', transform=get_val_test_transform_CLAHE())\n    test_dataset = ODIRDataset('test', transform=get_val_test_transform_CLAHE())\n    \n    print(\"\\n Resume: Train:\", len(train_dataset_clahe), \"| Val:\", len(val_dataset), \"| Test:\", len(test_dataset))\n\ndef create_datasets_for_config(config_name):\n    \"\"\"\n    Criar train/val/test datasets para uma configura√ß√£o\n    \n    Args:\n        config_name: 'v1', 'v2', 'v3', ou 'v4'\n    \n    Returns:\n        dict com keys 'train', 'val', 'test'\n    \"\"\"\n    \n    train_transforms = {\n        'v1': get_v1_baseline_transform(),\n        'v2': get_v2_crop_only_transform(),\n        'v3': get_v3_crop_aug_transform(),\n        'v4': get_v4_full_pipeline_transform()\n    }\n    \n    val_test_transforms = {\n        'v1': get_val_test_transform(),\n        'v2': get_val_test_transform(),\n        'v3': get_val_test_transform(),\n        'v4': get_val_test_transform_CLAHE()\n    }\n    \n    print(f\"\\nüì¶ Criando datasets para config: {config_name.upper()}\")\n    \n    datasets = {\n        'train': ODIRDataset('train', transform=train_transforms[config_name]),\n        'val': ODIRDataset('val', transform=val_test_transforms[config_name]),\n        'test': ODIRDataset('test', transform=val_test_transforms[config_name])\n    }\n    \n    print(f\"‚úÖ Datasets {config_name} criados!\")\n    \n    return datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:48:21.404730Z","iopub.execute_input":"2026-01-03T11:48:21.405099Z","iopub.status.idle":"2026-01-03T11:48:22.009804Z","shell.execute_reply.started":"2026-01-03T11:48:21.405074Z","shell.execute_reply":"2026-01-03T11:48:22.009045Z"}},"outputs":[{"name":"stdout","text":"üöÄ Dispositivo: cuda\n  üìÇ train: 4474 pacients, 5732 images\n  üìÇ train: 4474 pacients, 5732 images\n  üìÇ val: 959 pacients, 1728 images\n  üìÇ test: 959 pacients, 1748 images\n\n Resume: Train: 4474 | Val: 959 | Test: 959\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"datasets_v1 = create_datasets_for_config('v1')\ndatasets_v2 = create_datasets_for_config('v2')\ndatasets_v3 = create_datasets_for_config('v3')\ndatasets_v4 = create_datasets_for_config('v4')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:48:22.010803Z","iopub.execute_input":"2026-01-03T11:48:22.011215Z","iopub.status.idle":"2026-01-03T11:48:22.174476Z","shell.execute_reply.started":"2026-01-03T11:48:22.011181Z","shell.execute_reply":"2026-01-03T11:48:22.173878Z"}},"outputs":[{"name":"stdout","text":"\nüì¶ Criando datasets para config: V1\n  üìÇ train: 4474 pacients, 5732 images\n  üìÇ val: 959 pacients, 1728 images\n  üìÇ test: 959 pacients, 1748 images\n‚úÖ Datasets v1 criados!\n\nüì¶ Criando datasets para config: V2\n  üìÇ train: 4474 pacients, 5732 images\n  üìÇ val: 959 pacients, 1728 images\n  üìÇ test: 959 pacients, 1748 images\n‚úÖ Datasets v2 criados!\n\nüì¶ Criando datasets para config: V3\n  üìÇ train: 4474 pacients, 5732 images\n  üìÇ val: 959 pacients, 1728 images\n  üìÇ test: 959 pacients, 1748 images\n‚úÖ Datasets v3 criados!\n\nüì¶ Criando datasets para config: V4\n  üìÇ train: 4474 pacients, 5732 images\n  üìÇ val: 959 pacients, 1728 images\n  üìÇ test: 959 pacients, 1748 images\n‚úÖ Datasets v4 criados!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train_v1 = datasets_v1['train']\nval_v1 = datasets_v1['val']\ntest_v1 = datasets_v1['test']\n\ntrain_v2 = datasets_v2['train']\nval_v2 = datasets_v2['val']\ntest_v2 = datasets_v2['test']\n\ntrain_v3 = datasets_v3['train']\nval_v3 = datasets_v3['val']\ntest_v3 = datasets_v3['test']\n\ntrain_v4 = datasets_v4['train']\nval_v4 = datasets_v4['val']\ntest_v4 = datasets_v4['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:48:22.175254Z","iopub.execute_input":"2026-01-03T11:48:22.175532Z","iopub.status.idle":"2026-01-03T11:48:22.180209Z","shell.execute_reply.started":"2026-01-03T11:48:22.175500Z","shell.execute_reply":"2026-01-03T11:48:22.179402Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MobileNetV3Model(nn.Module):\n    \"\"\"\n    MobileNetV3-Large\n    - Moderno e eficiente\n    - R√°pido (2-3x mais que ResNet)\n    - Funciona bem em medical imaging\n    \"\"\"\n    \n    def __init__(self, num_classes=8, dropout=0.2):\n        super().__init__()\n        \n        # Carregar pretrained\n        self.model = timm.create_model('mobilenetv3_large_100', pretrained=True)\n        \n        # Substituir classifier\n        num_features = self.model.classifier.in_features\n        \n        self.model.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(num_features, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:48:22.181034Z","iopub.execute_input":"2026-01-03T11:48:22.181258Z","iopub.status.idle":"2026-01-03T11:48:22.197947Z","shell.execute_reply.started":"2026-01-03T11:48:22.181237Z","shell.execute_reply":"2026-01-03T11:48:22.197184Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(\"\\nüìä Model Information:\")\nmodel_test = MobileNetV3Model()\ntotal_params = sum(p.numel() for p in model_test.parameters())\ntrainable_params = sum(p.numel() for p in model_test.parameters() if p.requires_grad)\n\nprint(f\"  Total parameters:     {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\nprint(f\"  Model size:           ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n\ndel model_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:48:22.199839Z","iopub.execute_input":"2026-01-03T11:48:22.200316Z","iopub.status.idle":"2026-01-03T11:48:23.276420Z","shell.execute_reply.started":"2026-01-03T11:48:22.200282Z","shell.execute_reply":"2026-01-03T11:48:23.275658Z"}},"outputs":[{"name":"stdout","text":"\nüìä Model Information:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/22.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0ee550e8b3e4ad281e06c69e89d7429"}},"metadata":{}},{"name":"stdout","text":"  Total parameters:     4,212,280\n  Trainable parameters: 4,212,280\n  Model size:           ~16.1 MB\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(\"\\nüìÇ Using V4 preprocessing (CLAHE + Full Pipeline)...\")\n\n# Reusar datasets de V4\ntrain_mobile = train_v4  # Mesmo preprocessing!\nval_mobile = val_v4\ntest_mobile = test_v4\n\nprint(f\"‚úÖ Datasets ready:\")\nprint(f\"   Train: {len(train_mobile)} samples\")\nprint(f\"   Val:   {len(val_mobile)} samples\")\nprint(f\"   Test:  {len(test_mobile)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:48:23.277444Z","iopub.execute_input":"2026-01-03T11:48:23.277683Z","iopub.status.idle":"2026-01-03T11:48:23.282452Z","shell.execute_reply.started":"2026-01-03T11:48:23.277663Z","shell.execute_reply":"2026-01-03T11:48:23.281878Z"}},"outputs":[{"name":"stdout","text":"\nüìÇ Using V4 preprocessing (CLAHE + Full Pipeline)...\n‚úÖ Datasets ready:\n   Train: 4474 samples\n   Val:   959 samples\n   Test:  959 samples\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def train_model_complete(train_dataset, val_dataset, test_dataset, \n                        config_name, num_epochs=10, batch_size=32, lr=1e-4, use_class_weights=False, use_focal_loss=False):\n    \"\"\"\n    Treinar MOBILENET-V3 com visualiza√ß√µes autom√°ticas para o relat√≥rio\n    \"\"\"\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"TRAINING MOBILENET-V3 - {config_name.upper()}\")\n    print(f\"{'='*70}\\n\")\n    \n    # Setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    pin= True\n    print(f\"\\nüöÄ Device: {device}\")\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n                              shuffle=True, num_workers=2, pin_memory=pin)\n    val_loader   = DataLoader(val_dataset, batch_size=batch_size,\n                              shuffle=False, num_workers=2, pin_memory=pin)\n    test_loader  = DataLoader(test_dataset, batch_size=batch_size,\n                              shuffle=False, num_workers=2, pin_memory=pin)\n\n    # Model\n    model = MobileNetV3Model()\n    model = model.to(device)\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"üìä Model: {num_params:,} parameters\")\n\n    # ==================== LOSS COM CLASS WEIGHTS ====================\n    if use_class_weights:\n        print(\"\\nüîß Using CLIPPED CLASS WEIGHTS to balance classes...\")\n        class_weights = calculate_class_weights_clipped(  # ‚Üê USAR FUN√á√ÉO NOVA!\n            train_dataset, \n            max_weight=3.0  # ‚Üê H ser√° 2.38 (n√£o 5.65!)\n        ).to(device)\n        criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n        print(\"‚úÖ Clipped class weights activated!\")\n    else:\n        criterion = nn.BCEWithLogitsLoss()\n        print(\"‚ö†Ô∏è  Using standard BCE (no class weights)\")\n\n    criterion = criterion.to(device)\n    \n    # Optimizer\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr,           \n        weight_decay=1e-4  \n    )    \n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=5\n    )\n    \n    # Tracking\n    history = {\n        'train_loss': [], 'train_f1': [], 'train_acc': [],\n        'val_loss': [], 'val_f1': [], 'val_acc': [],\n        'learning_rates': [], 'epoch_times': []\n    }\n    \n    best_val_f1 = 0.0\n    start_time = time.time()\n    \n    # ==================== TRAINING LOOP ====================\n    for epoch in range(num_epochs):\n        epoch_start = time.time()\n        \n        # TRAIN\n        model.train()\n        train_loss = 0.0\n        train_preds, train_labels = [], []\n        \n        for images, labels, _ in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]'):\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad(set_to_none=True)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            train_preds.append(torch.sigmoid(outputs).detach().cpu())\n            train_labels.append(labels.cpu())\n        \n        train_loss /= len(train_loader)\n        train_preds = torch.cat(train_preds).numpy()\n        train_labels = torch.cat(train_labels).numpy()\n        train_preds_binary = (train_preds > 0.5).astype(int)\n        \n        train_f1 = f1_score(train_labels, train_preds_binary, average='macro', zero_division=0)\n        train_acc = 1.0 - hamming_loss(train_labels, train_preds_binary)\n        \n        # VALIDATION\n        model.eval()\n        val_loss = 0.0\n        val_preds, val_labels = [], []\n        \n        with torch.no_grad():\n            for images, labels, _ in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]'):\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                val_preds.append(torch.sigmoid(outputs).cpu())\n                val_labels.append(labels.cpu())\n        \n        val_loss /= len(val_loader)\n        val_preds = torch.cat(val_preds).numpy()\n        val_labels = torch.cat(val_labels).numpy()\n        val_preds_binary = (val_preds > 0.5).astype(int)\n        \n        val_f1 = f1_score(val_labels, val_preds_binary, average='macro', zero_division=0)\n        val_acc   = 1.0 - hamming_loss(val_labels, val_preds_binary)\n        \n        # Scheduler\n        scheduler.step(val_f1)\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # Save history\n        epoch_time = time.time() - epoch_start\n        history['train_loss'].append(train_loss)\n        history['train_f1'].append(train_f1)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_f1'].append(val_f1)\n        history['val_acc'].append(val_acc)\n        history['learning_rates'].append(current_lr)\n        history['epoch_times'].append(epoch_time)\n        \n        # Print progress\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n        print(f\"  Train - Loss: {train_loss:.4f}, F1: {train_f1:.4f}, Acc: {train_acc:.4f}\")\n        print(f\"  Val   - Loss: {val_loss:.4f}, F1: {val_f1:.4f}, Acc: {val_acc:.4f}\")\n        print(f\"  LR: {current_lr:.2e}, Time: {epoch_time:.1f}s\")\n        \n        # Save best model\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_f1': val_f1,\n                'history': history\n            }, f'efficientnet_{config_name}_best.pth')\n            print(f\"  ‚úÖ Best model saved! (F1: {val_f1:.4f})\")\n    \n    total_train_time = time.time() - start_time\n    \n    # ==================== TEST EVALUATION ====================\n    print(f\"\\n{'='*70}\")\n    print(\"EVALUATING ON TEST SET\")\n    print(f\"{'='*70}\\n\")\n    \n    checkpoint = torch.load(f'efficientnet_{config_name}_best.pth')\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    \n    test_preds, test_labels = [], []\n    inference_times = []\n    \n    with torch.no_grad():\n        for images, labels, _ in tqdm(test_loader, desc='Testing'):\n            images, labels = images.to(device), labels.to(device)\n            \n            start = time.time()\n            outputs = model(images)\n            inference_times.append((time.time() - start) / len(images))\n            \n            test_preds.append(torch.sigmoid(outputs).cpu())\n            test_labels.append(labels.cpu())\n    \n    test_preds = torch.cat(test_preds).numpy()\n    test_labels = torch.cat(test_labels).numpy()\n    test_preds_binary = (test_preds > 0.5).astype(int)\n    \n    # Metrics\n    test_f1 = f1_score(test_labels, test_preds_binary, average='macro', zero_division=0)\n    test_acc  = 1.0 - hamming_loss(test_labels, test_preds_binary)\n    kappas = [\n        cohen_kappa_score(test_labels[:, i], test_preds_binary[:, i])\n        for i in range(test_labels.shape[1])\n    ]\n    test_kappa = float(np.mean(kappas))\n    \n    # Per-class metrics\n    class_names = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n    per_class_f1 = f1_score(test_labels, test_preds_binary, average=None, zero_division=0)\n    \n    print(f\"\\nüìä FINAL TEST RESULTS - {config_name.upper()}:\")\n    print(f\"  Accuracy:  {test_acc:.4f}\")\n    print(f\"  F1-Score:  {test_f1:.4f}\")\n    print(f\"  Kappa:     {test_kappa:.4f}\")\n    print(f\"  Avg Epoch Time: {np.mean(history['epoch_times']):.1f}s\")\n    print(f\"  Avg Inference:  {np.mean(inference_times)*1000:.2f}ms/image\")\n    print(f\"\\n  Per-class F1:\")\n    for i, name in enumerate(class_names):\n        print(f\"    {name}: {per_class_f1[i]:.4f}\")\n    \n    # ==================== SAVE RESULTS ====================\n    results = {\n        'config': config_name,\n        'num_params': num_params,\n        'num_epochs': num_epochs,\n        'best_val_f1': float(best_val_f1),\n        'test_accuracy': float(test_acc),\n        'test_f1': float(test_f1),\n        'test_kappa': float(test_kappa),\n        'per_class_f1': {name: float(f1) for name, f1 in zip(class_names, per_class_f1)},\n        'total_train_time': float(total_train_time),\n        'avg_epoch_time': float(np.mean(history['epoch_times'])),\n        'avg_inference_time_ms': float(np.mean(inference_times) * 1000)\n    }\n    \n    with open(f'efficientnet_{config_name}_results.json', 'w') as f:\n        json.dump(results, f, indent=2)\n    \n    # Save history\n    np.save(f'efficientnet_{config_name}_history.npy', history)\n    \n    # ==================== GENERATE VISUALIZATIONS ====================\n    print(f\"\\nüìä Generating visualizations for report...\")\n    \n    # 1. TRAINING CURVES\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    fig.suptitle(f'EfficientNet-B0 {config_name.upper()} - Training Progress', \n                 fontsize=16, fontweight='bold')\n    \n    epochs = range(1, len(history['train_loss']) + 1)\n    \n    # Loss\n    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].set_title('Training & Validation Loss')\n    axes[0, 0].legend()\n    axes[0, 0].grid(alpha=0.3)\n    \n    # F1-Score\n    axes[0, 1].plot(epochs, history['train_f1'], 'b-', label='Train F1', linewidth=2)\n    axes[0, 1].plot(epochs, history['val_f1'], 'r-', label='Val F1', linewidth=2)\n    axes[0, 1].axhline(y=best_val_f1, color='g', linestyle='--', \n                       label=f'Best Val F1: {best_val_f1:.4f}')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('F1-Score')\n    axes[0, 1].set_title('F1-Score Progress')\n    axes[0, 1].legend()\n    axes[0, 1].grid(alpha=0.3)\n    \n    # Accuracy\n    axes[1, 0].plot(epochs, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n    axes[1, 0].plot(epochs, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n    axes[1, 0].set_xlabel('Epoch')\n    axes[1, 0].set_ylabel('Accuracy')\n    axes[1, 0].set_title('Accuracy Progress')\n    axes[1, 0].legend()\n    axes[1, 0].grid(alpha=0.3)\n    \n    # Learning Rate\n    axes[1, 1].plot(epochs, history['learning_rates'], 'purple', linewidth=2)\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('Learning Rate')\n    axes[1, 1].set_title('Learning Rate Schedule')\n    axes[1, 1].set_yscale('log')\n    axes[1, 1].grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(f'efficientnet_{config_name}_training_curves.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    # 2. CONFUSION MATRIX (multi-label)\n    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n    fig.suptitle(f'{config_name.upper()} - Per-Class Confusion Matrices', \n                 fontsize=16, fontweight='bold')\n    \n    for i, class_name in enumerate(class_names):\n        ax = axes[i // 4, i % 4]\n        cm = confusion_matrix(test_labels[:, i], test_preds_binary[:, i])\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n                   xticklabels=['Neg', 'Pos'], yticklabels=['Neg', 'Pos'])\n        ax.set_title(f'{class_name} (F1: {per_class_f1[i]:.3f})')\n        ax.set_xlabel('Predicted')\n        ax.set_ylabel('True')\n    \n    plt.tight_layout()\n    plt.savefig(f'efficientnet_{config_name}_confusion_matrices.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    # 3. PER-CLASS F1 BAR CHART\n    fig, ax = plt.subplots(figsize=(10, 6))\n    bars = ax.bar(class_names, per_class_f1, color='steelblue', edgecolor='black')\n    ax.axhline(y=test_f1, color='red', linestyle='--', linewidth=2, \n               label=f'Average F1: {test_f1:.4f}')\n    ax.set_xlabel('Disease Class', fontsize=12)\n    ax.set_ylabel('F1-Score', fontsize=12)\n    ax.set_title(f'{config_name.upper()} - Per-Class Performance', \n                 fontsize=14, fontweight='bold')\n    ax.set_ylim([0, 1.0])\n    ax.legend()\n    ax.grid(axis='y', alpha=0.3)\n    \n    # Add value labels on bars\n    for bar, f1 in zip(bars, per_class_f1):\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n                f'{f1:.3f}', ha='center', va='bottom', fontsize=10)\n    \n    plt.tight_layout()\n    plt.savefig(f'efficientnet_{config_name}_per_class_f1.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    print(f\"‚úÖ Visualizations saved:\")\n    print(f\"   - efficientnet_{config_name}_training_curves.png\")\n    print(f\"   - efficientnet_{config_name}_confusion_matrices.png\")\n    print(f\"   - efficientnet_{config_name}_per_class_f1.png\")\n    print(f\"   - efficientnet_{config_name}_results.json\")\n    print(f\"   - efficientnet_{config_name}_history.npy\")\n    \n    return results, model, history\n\nprint(\"‚úÖ Training function with visualizations ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:48:23.283509Z","iopub.execute_input":"2026-01-03T11:48:23.283726Z","iopub.status.idle":"2026-01-03T11:48:23.318304Z","shell.execute_reply.started":"2026-01-03T11:48:23.283707Z","shell.execute_reply":"2026-01-03T11:48:23.317694Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Training function with visualizations ready!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(\"\\n Starting training...\")\nprint(\"=\"*80 + \"\\n\")\n\n# TREINAR (reusar fun√ß√£o existente)\nresults_mobile, model_mobile, history_mobile = train_model_complete(\n    train_mobile, \n    val_mobile, \n    test_mobile,\n    config_name='mobilenetv3_large',\n    num_epochs=40,\n    batch_size=32,\n    lr=1e-4,\n    use_class_weights=False,\n    use_focal_loss=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:48:23.319207Z","iopub.execute_input":"2026-01-03T11:48:23.319488Z","iopub.status.idle":"2026-01-03T12:32:27.979945Z","shell.execute_reply.started":"2026-01-03T11:48:23.319457Z","shell.execute_reply":"2026-01-03T12:32:27.979195Z"}},"outputs":[{"name":"stdout","text":"\n Starting training...\n================================================================================\n\n\n======================================================================\nTRAINING MOBILENET-V3 - MOBILENETV3_LARGE\n======================================================================\n\n\nüöÄ Device: cuda\nüìä Model: 4,212,280 parameters\n‚ö†Ô∏è  Using standard BCE (no class weights)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [01:04<00:00,  2.19it/s]\nEpoch 1/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:13<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/40:\n  Train - Loss: 0.3604, F1: 0.0869, Acc: 0.8499\n  Val   - Loss: 0.3012, F1: 0.2299, Acc: 0.8693\n  LR: 1.00e-04, Time: 78.0s\n  ‚úÖ Best model saved! (F1: 0.2299)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:52<00:00,  2.67it/s]\nEpoch 2/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/40:\n  Train - Loss: 0.2856, F1: 0.2934, Acc: 0.8723\n  Val   - Loss: 0.2711, F1: 0.3844, Acc: 0.8797\n  LR: 1.00e-04, Time: 63.1s\n  ‚úÖ Best model saved! (F1: 0.3844)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:52<00:00,  2.69it/s]\nEpoch 3/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/40:\n  Train - Loss: 0.2569, F1: 0.4166, Acc: 0.8849\n  Val   - Loss: 0.2558, F1: 0.4571, Acc: 0.8856\n  LR: 1.00e-04, Time: 62.5s\n  ‚úÖ Best model saved! (F1: 0.4571)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:52<00:00,  2.67it/s]\nEpoch 4/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/40:\n  Train - Loss: 0.2309, F1: 0.5012, Acc: 0.8978\n  Val   - Loss: 0.2437, F1: 0.5086, Acc: 0.8904\n  LR: 1.00e-04, Time: 63.0s\n  ‚úÖ Best model saved! (F1: 0.5086)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:51<00:00,  2.69it/s]\nEpoch 5/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/40:\n  Train - Loss: 0.2052, F1: 0.5772, Acc: 0.9093\n  Val   - Loss: 0.2412, F1: 0.5289, Acc: 0.8959\n  LR: 1.00e-04, Time: 62.3s\n  ‚úÖ Best model saved! (F1: 0.5289)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:51<00:00,  2.74it/s]\nEpoch 6/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/40:\n  Train - Loss: 0.1787, F1: 0.6390, Acc: 0.9229\n  Val   - Loss: 0.2212, F1: 0.6360, Acc: 0.9090\n  LR: 1.00e-04, Time: 61.4s\n  ‚úÖ Best model saved! (F1: 0.6360)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:51<00:00,  2.72it/s]\nEpoch 7/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/40:\n  Train - Loss: 0.1515, F1: 0.7130, Acc: 0.9371\n  Val   - Loss: 0.2131, F1: 0.6906, Acc: 0.9187\n  LR: 1.00e-04, Time: 61.8s\n  ‚úÖ Best model saved! (F1: 0.6906)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:51<00:00,  2.70it/s]\nEpoch 8/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/40:\n  Train - Loss: 0.1258, F1: 0.7703, Acc: 0.9493\n  Val   - Loss: 0.2113, F1: 0.7368, Acc: 0.9230\n  LR: 1.00e-04, Time: 62.3s\n  ‚úÖ Best model saved! (F1: 0.7368)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:51<00:00,  2.73it/s]\nEpoch 9/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/40:\n  Train - Loss: 0.1099, F1: 0.8178, Acc: 0.9561\n  Val   - Loss: 0.2070, F1: 0.7484, Acc: 0.9290\n  LR: 1.00e-04, Time: 61.6s\n  ‚úÖ Best model saved! (F1: 0.7484)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:52<00:00,  2.68it/s]\nEpoch 10/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/40:\n  Train - Loss: 0.0936, F1: 0.8487, Acc: 0.9635\n  Val   - Loss: 0.2152, F1: 0.7641, Acc: 0.9320\n  LR: 1.00e-04, Time: 62.5s\n  ‚úÖ Best model saved! (F1: 0.7641)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:51<00:00,  2.73it/s]\nEpoch 11/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/40:\n  Train - Loss: 0.0781, F1: 0.8682, Acc: 0.9703\n  Val   - Loss: 0.2105, F1: 0.7883, Acc: 0.9356\n  LR: 1.00e-04, Time: 61.6s\n  ‚úÖ Best model saved! (F1: 0.7883)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:53<00:00,  2.61it/s]\nEpoch 12/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/40:\n  Train - Loss: 0.0675, F1: 0.8989, Acc: 0.9749\n  Val   - Loss: 0.2110, F1: 0.7932, Acc: 0.9378\n  LR: 1.00e-04, Time: 64.6s\n  ‚úÖ Best model saved! (F1: 0.7932)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:57<00:00,  2.43it/s]\nEpoch 13/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:11<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/40:\n  Train - Loss: 0.0574, F1: 0.9098, Acc: 0.9790\n  Val   - Loss: 0.2167, F1: 0.8036, Acc: 0.9402\n  LR: 1.00e-04, Time: 69.3s\n  ‚úÖ Best model saved! (F1: 0.8036)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:59<00:00,  2.37it/s]\nEpoch 14/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:11<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/40:\n  Train - Loss: 0.0542, F1: 0.9201, Acc: 0.9804\n  Val   - Loss: 0.2238, F1: 0.8168, Acc: 0.9400\n  LR: 1.00e-04, Time: 70.4s\n  ‚úÖ Best model saved! (F1: 0.8168)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:56<00:00,  2.46it/s]\nEpoch 15/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:11<00:00,  2.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15/40:\n  Train - Loss: 0.0468, F1: 0.9402, Acc: 0.9839\n  Val   - Loss: 0.2242, F1: 0.8213, Acc: 0.9423\n  LR: 1.00e-04, Time: 68.4s\n  ‚úÖ Best model saved! (F1: 0.8213)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:56<00:00,  2.47it/s]\nEpoch 16/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:11<00:00,  2.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16/40:\n  Train - Loss: 0.0406, F1: 0.9464, Acc: 0.9855\n  Val   - Loss: 0.2200, F1: 0.8145, Acc: 0.9426\n  LR: 1.00e-04, Time: 67.8s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:55<00:00,  2.54it/s]\nEpoch 17/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:11<00:00,  2.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17/40:\n  Train - Loss: 0.0395, F1: 0.9461, Acc: 0.9855\n  Val   - Loss: 0.2212, F1: 0.8270, Acc: 0.9436\n  LR: 1.00e-04, Time: 66.2s\n  ‚úÖ Best model saved! (F1: 0.8270)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:55<00:00,  2.54it/s]\nEpoch 18/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:11<00:00,  2.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 18/40:\n  Train - Loss: 0.0319, F1: 0.9584, Acc: 0.9887\n  Val   - Loss: 0.2307, F1: 0.8315, Acc: 0.9453\n  LR: 1.00e-04, Time: 66.3s\n  ‚úÖ Best model saved! (F1: 0.8315)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.57it/s]\nEpoch 19/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 19/40:\n  Train - Loss: 0.0308, F1: 0.9604, Acc: 0.9889\n  Val   - Loss: 0.2304, F1: 0.8334, Acc: 0.9496\n  LR: 1.00e-04, Time: 65.6s\n  ‚úÖ Best model saved! (F1: 0.8334)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.55it/s]\nEpoch 20/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:11<00:00,  2.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20/40:\n  Train - Loss: 0.0258, F1: 0.9646, Acc: 0.9912\n  Val   - Loss: 0.2419, F1: 0.8369, Acc: 0.9481\n  LR: 1.00e-04, Time: 65.9s\n  ‚úÖ Best model saved! (F1: 0.8369)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.55it/s]\nEpoch 21/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 21/40:\n  Train - Loss: 0.0253, F1: 0.9701, Acc: 0.9911\n  Val   - Loss: 0.2521, F1: 0.8427, Acc: 0.9486\n  LR: 1.00e-04, Time: 65.9s\n  ‚úÖ Best model saved! (F1: 0.8427)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.55it/s]\nEpoch 22/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:11<00:00,  2.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 22/40:\n  Train - Loss: 0.0254, F1: 0.9673, Acc: 0.9908\n  Val   - Loss: 0.2484, F1: 0.8296, Acc: 0.9447\n  LR: 1.00e-04, Time: 66.0s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.57it/s]\nEpoch 23/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 23/40:\n  Train - Loss: 0.0225, F1: 0.9734, Acc: 0.9925\n  Val   - Loss: 0.2469, F1: 0.8336, Acc: 0.9479\n  LR: 1.00e-04, Time: 65.4s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.56it/s]\nEpoch 24/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 24/40:\n  Train - Loss: 0.0198, F1: 0.9774, Acc: 0.9931\n  Val   - Loss: 0.2479, F1: 0.8376, Acc: 0.9501\n  LR: 1.00e-04, Time: 65.5s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.57it/s]\nEpoch 25/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 25/40:\n  Train - Loss: 0.0180, F1: 0.9769, Acc: 0.9941\n  Val   - Loss: 0.2714, F1: 0.8353, Acc: 0.9475\n  LR: 1.00e-04, Time: 65.3s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.57it/s]\nEpoch 26/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 26/40:\n  Train - Loss: 0.0162, F1: 0.9832, Acc: 0.9944\n  Val   - Loss: 0.2701, F1: 0.8385, Acc: 0.9477\n  LR: 1.00e-04, Time: 65.2s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.58it/s]\nEpoch 27/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 27/40:\n  Train - Loss: 0.0175, F1: 0.9793, Acc: 0.9935\n  Val   - Loss: 0.2822, F1: 0.8382, Acc: 0.9484\n  LR: 5.00e-05, Time: 65.2s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.58it/s]\nEpoch 28/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 28/40:\n  Train - Loss: 0.0143, F1: 0.9842, Acc: 0.9953\n  Val   - Loss: 0.2640, F1: 0.8386, Acc: 0.9476\n  LR: 5.00e-05, Time: 65.1s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.57it/s]\nEpoch 29/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 29/40:\n  Train - Loss: 0.0110, F1: 0.9900, Acc: 0.9966\n  Val   - Loss: 0.2657, F1: 0.8371, Acc: 0.9484\n  LR: 5.00e-05, Time: 65.4s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.57it/s]\nEpoch 30/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 30/40:\n  Train - Loss: 0.0102, F1: 0.9894, Acc: 0.9970\n  Val   - Loss: 0.2689, F1: 0.8365, Acc: 0.9492\n  LR: 5.00e-05, Time: 65.5s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.55it/s]\nEpoch 31/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 31/40:\n  Train - Loss: 0.0086, F1: 0.9908, Acc: 0.9971\n  Val   - Loss: 0.2766, F1: 0.8443, Acc: 0.9490\n  LR: 5.00e-05, Time: 65.7s\n  ‚úÖ Best model saved! (F1: 0.8443)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.59it/s]\nEpoch 32/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 32/40:\n  Train - Loss: 0.0072, F1: 0.9927, Acc: 0.9979\n  Val   - Loss: 0.2700, F1: 0.8448, Acc: 0.9496\n  LR: 5.00e-05, Time: 65.1s\n  ‚úÖ Best model saved! (F1: 0.8448)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.56it/s]\nEpoch 33/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 33/40:\n  Train - Loss: 0.0082, F1: 0.9909, Acc: 0.9974\n  Val   - Loss: 0.2695, F1: 0.8407, Acc: 0.9499\n  LR: 5.00e-05, Time: 65.6s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.58it/s]\nEpoch 34/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 34/40:\n  Train - Loss: 0.0076, F1: 0.9919, Acc: 0.9974\n  Val   - Loss: 0.2749, F1: 0.8528, Acc: 0.9515\n  LR: 5.00e-05, Time: 65.2s\n  ‚úÖ Best model saved! (F1: 0.8528)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.58it/s]\nEpoch 35/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 35/40:\n  Train - Loss: 0.0074, F1: 0.9915, Acc: 0.9974\n  Val   - Loss: 0.2929, F1: 0.8438, Acc: 0.9493\n  LR: 5.00e-05, Time: 65.3s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.55it/s]\nEpoch 36/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 36/40:\n  Train - Loss: 0.0068, F1: 0.9931, Acc: 0.9978\n  Val   - Loss: 0.2820, F1: 0.8543, Acc: 0.9502\n  LR: 5.00e-05, Time: 65.8s\n  ‚úÖ Best model saved! (F1: 0.8543)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:54<00:00,  2.56it/s]\nEpoch 37/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 37/40:\n  Train - Loss: 0.0066, F1: 0.9946, Acc: 0.9978\n  Val   - Loss: 0.2982, F1: 0.8482, Acc: 0.9511\n  LR: 5.00e-05, Time: 65.5s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:56<00:00,  2.49it/s]\nEpoch 38/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:11<00:00,  2.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 38/40:\n  Train - Loss: 0.0072, F1: 0.9928, Acc: 0.9976\n  Val   - Loss: 0.2935, F1: 0.8374, Acc: 0.9493\n  LR: 5.00e-05, Time: 67.7s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:56<00:00,  2.48it/s]\nEpoch 39/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:11<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 39/40:\n  Train - Loss: 0.0070, F1: 0.9927, Acc: 0.9977\n  Val   - Loss: 0.2996, F1: 0.8441, Acc: 0.9492\n  LR: 5.00e-05, Time: 68.0s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/40 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:56<00:00,  2.49it/s]\nEpoch 40/40 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:10<00:00,  2.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 40/40:\n  Train - Loss: 0.0083, F1: 0.9916, Acc: 0.9972\n  Val   - Loss: 0.2988, F1: 0.8505, Acc: 0.9496\n  LR: 5.00e-05, Time: 67.0s\n\n======================================================================\nEVALUATING ON TEST SET\n======================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:15<00:00,  1.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä FINAL TEST RESULTS - MOBILENETV3_LARGE:\n  Accuracy:  0.9572\n  F1-Score:  0.8574\n  Kappa:     0.8287\n  Avg Epoch Time: 65.5s\n  Avg Inference:  0.38ms/image\n\n  Per-class F1:\n    N: 0.8624\n    D: 0.8784\n    G: 0.8480\n    C: 0.9167\n    A: 0.8571\n    H: 0.8000\n    M: 0.9286\n    O: 0.7679\n\nüìä Generating visualizations for report...\n‚úÖ Visualizations saved:\n   - efficientnet_mobilenetv3_large_training_curves.png\n   - efficientnet_mobilenetv3_large_confusion_matrices.png\n   - efficientnet_mobilenetv3_large_per_class_f1.png\n   - efficientnet_mobilenetv3_large_results.json\n   - efficientnet_mobilenetv3_large_history.npy\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"üìä MOBILENETV3 vs V4 COMPARISON\")\nprint(\"=\"*80)\n\nmobile_f1 = results_mobile['test_f1']\nv4_f1 = 0.8659  # V4 default\n\nprint(f\"\\nüèÜ TEST F1-SCORES:\")\nprint(f\"  V4 (EfficientNet-B0):      {v4_f1:.4f}\")\nprint(f\"  MobileNetV3-Large:         {mobile_f1:.4f}\")\nprint(f\"  Difference:                {mobile_f1 - v4_f1:+.4f} ({(mobile_f1/v4_f1 - 1)*100:+.1f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T12:32:27.981010Z","iopub.execute_input":"2026-01-03T12:32:27.981295Z","iopub.status.idle":"2026-01-03T12:32:27.986978Z","shell.execute_reply.started":"2026-01-03T12:32:27.981268Z","shell.execute_reply":"2026-01-03T12:32:27.986276Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüìä MOBILENETV3 vs V4 COMPARISON\n================================================================================\n\nüèÜ TEST F1-SCORES:\n  V4 (EfficientNet-B0):      0.8659\n  MobileNetV3-Large:         0.8574\n  Difference:                -0.0085 (-1.0%)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"if mobile_f1 > 0.85:\n    print(f\"\\n{'='*80}\")\n    print(\"‚úÖ MOBILENETV3 PERFORMANCE GOOD!\")\n    print(f\"{'='*80}\")\n    print(f\"MobileNetV3 achieved F1 > 0.85: {mobile_f1:.4f}\")\n    print(\"Ensemble with V4 is RECOMMENDED!\")\n    print(\"\\nExpected ensemble F1: 0.87-0.88\")\n    print(\"Proceed to ensemble cell below!\")\n    \nelif mobile_f1 > 0.80:\n    print(f\"\\n{'='*80}\")\n    print(\"ü§î MOBILENETV3 PERFORMANCE OK\")\n    print(f\"{'='*80}\")\n    print(f\"MobileNetV3 achieved F1 = {mobile_f1:.4f}\")\n    print(\"Ensemble MIGHT help, but uncertain.\")\n    print(\"Test ensemble to see if improves over V4.\")\n    \nelse:\n    print(f\"\\n{'='*80}\")\n    print(\"‚ö†Ô∏è  MOBILENETV3 PERFORMANCE WEAK\")\n    print(f\"{'='*80}\")\n    print(f\"MobileNetV3 achieved F1 = {mobile_f1:.4f}\")\n    print(\"Ensemble NOT recommended (would hurt performance).\")\n    print(\"Use V4 default (0.8659) as final result.\")\n\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T12:32:27.987998Z","iopub.execute_input":"2026-01-03T12:32:27.988257Z","iopub.status.idle":"2026-01-03T12:32:28.005248Z","shell.execute_reply.started":"2026-01-03T12:32:27.988235Z","shell.execute_reply":"2026-01-03T12:32:28.004655Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\n‚úÖ MOBILENETV3 PERFORMANCE GOOD!\n================================================================================\nMobileNetV3 achieved F1 > 0.85: 0.8574\nEnsemble with V4 is RECOMMENDED!\n\nExpected ensemble F1: 0.87-0.88\nProceed to ensemble cell below!\n================================================================================\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import json\n\ncomparison_results = {\n    'v4_efficientnet': {\n        'test_f1': v4_f1,\n        'config': 'EfficientNet-B0 + CLAHE + Full Pipeline'\n    },\n    'mobilenetv3': {\n        'test_f1': mobile_f1,\n        'config': 'MobileNetV3-Large + CLAHE + Full Pipeline',\n        'per_class_f1': results_mobile['per_class_f1']\n    },\n    'comparison': {\n        'difference': float(mobile_f1 - v4_f1),\n        'percent_change': float((mobile_f1/v4_f1 - 1)*100),\n        'best_model': 'mobilenetv3' if mobile_f1 > v4_f1 else 'v4'\n    }\n}\n\nwith open('/kaggle/working/model_comparison_v4_vs_mobile.json', 'w') as f:\n    json.dump(comparison_results, f, indent=2)\n\nprint(\"\\n‚úÖ Comparison saved: model_comparison_v4_vs_mobile.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T12:32:28.006119Z","iopub.execute_input":"2026-01-03T12:32:28.006698Z","iopub.status.idle":"2026-01-03T12:32:28.026016Z","shell.execute_reply.started":"2026-01-03T12:32:28.006661Z","shell.execute_reply":"2026-01-03T12:32:28.025499Z"}},"outputs":[{"name":"stdout","text":"\n‚úÖ Comparison saved: model_comparison_v4_vs_mobile.json\n","output_type":"stream"}],"execution_count":12}]}